Title         : Tracking and Re-Identification System for Multiple Laboratory Animals

#Author        : Filip Naiser, Matěj Šmíd, Jiří Matas
#Affiliation   : Center for Machine Perception, Czech Technical University in Prague, Czech Republic
#Email         : filip@naiser.cz, {smidm,matas}@cmp.felk.cvut.cz

Logo          : True

Doc class     : [a4paper,conference]IEEEtran
Bibliography  : library.bib

[TITLE]

~ Abstract

We present a tracking system for ecology and biology researchers suitable for movement and interaction analysis of multiple animals in laboratory conditions. On the input is a single video with multiple animals and the outputs are animal trajectories. The system is agnostic with regard to animal species. The detection and re-identification modules are learnable and can be adopted to a new animal appearance automatically without annotation. For animal re-identification we use discriminatively trained CNN embedding. Ambiguous tracking situations when animals interact are solved using a special CNN based detector trained on synthetic interactions. The system was tested on sequences with multiple ants, zebrafish and sowbugs.

TODO human in the loop?

<!--
Tracking laboratory animals for high-throughput ecology and biology research has different goals and properties than well researched people and specifically pedestrian tracking. The tracked objects are nearly indistinguishable, the scenes are mostly planar and viewed from top, the interactions are frequent etc. In this paper we present a tracking system that addresses the issues. The object re-identification is based on Siamese CNN descriptors trained with contrastive loss. Instead of treating interactions as gaps in tracks, we developed a special tracker for objects in interactions. Unreliable tracking decisions are deferred and optionally left for a human to decide. The presented tracking system has GUI for end user interaction and will be published as open source.
-->
~

# Introduction

Image-based animal tracking recently enabled high-throughput methods in biology and ecology. Numerous insights were achieved using automated tracking [@Dell2014]. There are already several freely available tracking systems for end users [@Perez-Escudero2014, @Rodriquez2017, @Rodriquez2018, @Shin2018, @XU2017].

As stated in [@Dell2014], the range of possible tracking problem difficulty and output quality is broad in ecology research. The tracking task ranges from very simple laboratory habitat with few animals to a complex landscape with many individuals directly in field. The basic tracking systems provide output without individual identities and infer only animal positions. Advanced systems maintain identities and recover not only position, but also pose of an animal (e.g., direction of movement, body shape, legs or wings positions).

The presented system is suitable for laboratory habitat. The arena where animals move should have uniform background and constant lighting. The shape of the habitat can be arbitrary. Lighting and background conditions are not necessary required when acquiring image data in an other modality than visible light. It is expected that the camera is stationary. The tracking objects are often nearly indistinguishable animals of one species. The camera is in most cases observing the scene from above. The animals move on a plane or in shallow water. This ensures that that the animals are viewed from a single direction and the scale changes are negligible.

Distinguishable features of this work are learnable re-identification module, learnable interaction solver and focus on error minimization. When the system is not certain that a decision is correct, it leaves the affected part of the trajectories undecided or asks the user. The mentioned learnable modules are learned in an unsupervised way.

The re-learning is needed in case of change in appearance.

We based our work on [@Naiser2017]

The system is evaluated on four videos of ants, zebrafish and sowbugs. We compare our results with the established baseline idTracker [@Pere-Escudero2014].

<!--
input definition
    - x laboratory conditions
    - x uniform background
    - x uniform and constant lighting
    - x similar or nearly indistinguishable individuals
    - x small or no changes in scale due to planar or shallow scene
    - x view from above
    - multiple views: advantages / disadvantages

outputs in general
    - x detections no tracks
    - x tracks without id
    - x tracks with id

- motivation (viz diplomka Introduction A)
- why is preserve identity crucial
-->
The mentioned idTracker
<!--
- state of the art
  - idtracker [@Perez-Escudero2014]
    - re-identification: intensity map, contrast map
  - toxtrac [@Rodriquez2017]
    - realtime
    - bg subtraction, thresholding, morphologty closing, filtering
    - kalman, frame distance, size change
    - re-identification: intensity histogram, texture features
  - toxid [@Rodriquez2018]
    - matching matrix
    - histogram correlation
    - hungarian algorithm
  - abctracker [@Shin2018]
    - unlabeled data, hog features, transfer learning adaboost [@Nguyen2017]
  - zebrafish cnn [@XU2017]


- contributions
  - long term tracking with robust identity preservation (id re-identification)
  - tracking interactions
  - focus on high accuracy: do automatically confident decisions, employ human to the loop where needed
  - general learnable for multiple species
- overview + diagram

<!---
- agglomerative clustering in spatio-temporal space

code references:

- segmentation
    `mser.py: ferda_filtered_msers. parallelization.py`
- assembly_after_parallelization
    `core.bg_computer_assembling.assembly_after_parallelization`
- cardinality classification
    `gui.region_classifier_tool.RegionClassifierTool#start_hil`
- id detection
-->

# Methods { #sec-methods }

In all frames of the input video are first segmented regions with single or multiple objects. An initial tracking graph is constructed. Nodes represent regions and edges possible transitions between regions in consecutive frames. The graph is further pruned and modified in such way that only the most probable transitions between regions remain. The paths in graph where no branching occurs are joined into *tracklets*. The tracklets are then classified to categories *single animal*, *multiple animals*, *noise* and *animal part*. The classifier is learned from a few user supplied examples per category. Re-identification module and module for tracking in interactions are learned (when needed) on tracklet data in unsupervised way. The re-identification module computes a probability that two tracklets belong to the same individual. Multiple animals tracklets are then split to separate single animal tracklets using the interaction tracking module. Tracklets of the same individual are joined into tracks and the identity information is propagated in the tracking graph. The overview is illustrated in flowcharts on Figure [#fig-flowchart].

~ Figure { #fig-flowchart; caption:"Tracking system overview. The learning procesess are separated in the right box for clarity. The three learned models are used in the appropriate stages of the tracking process (as shown in the left box)." }
![flowchart]
~

[flowchart]: figures/flowchart-crop.pdf "tracking system flowchart" { width:auto; max-width:100%}
[flowchart_learning]: figures/flowchart_a-crop.pdf "tracking system flowchart (learning)" { width:auto; max-width:100%}
[flowchart_tracking]: figures/flowchart_b-crop.pdf "tracking system flowchart (tracking)" { width:auto; max-width:100%}

## Segmentation { #sec-segmentation }

sequence of regions

As a first step in the processing of a new video are the animals segmented from the background. The arenas in typical laboratory experiments have mostly uniform colours and are not cluttered. We were able to avoid complex object detectors and background subtraction algorithms and still achieved satisfactory results using *maximally stable extremal regions* (MSERs) [@Matas2004] algorithm. The regions are further filtered by multiple criteria: minimal and maximal area, minimal number of thresholds for which is the MSER region stable (MSER margin), nested regions removal and suppression of bright regions. An illustration of segmented regions after filtering is on Figure [#fig-mser].

~ Figure { #fig-mser; caption:"Segmentation of six ants by MSER algorithm. Four ants on the left form a group and are segmented as a single region. This example shows MSERs displayed after filtering out unsuitable regions."}
![mser]
~

[mser]: images/Cam1_clip_frame2778_mser_segmentation.png "MSER segmentation" { width:auto; max-width:90% }

<!---
- mser
- filtering
    - max area, min margin, min area
- currently no bg subtraction
- see `mser.py: ferda_filtered_msers, parallelization.py`
-->

## Tracking Graph { #sec-graph }

- all region in t+1
- spatial gating/pruning
- creating tracklets
    def.: identity set
- graph pruning, agglomerative clustering

## Region Cardinality Classification { #sec-cardinality }

- manual interaction
- features: `[r.area(), r.a_, r.b_, (4*self.major_axis_ * self.minor_axis_) / float(self.area())]`
- Figure 4.3 diplomka

~ Figure { #fig-cardinality; caption:"PLACEHOLDER for region cardinality classification illustration." }
![cardinality]
~

[cardinality]: figures/cardinality_examples.png "cardinality classification examples" { width:auto; max-width:90% }

<!---
```
annotated_data = examples of single id regions,
    multiple id regions and noise
learn normalization (centre data and scale to
    unit variance)
for each tracklet do:
    for each region in tracklet do:
        transform data
        euclidean nearest neighbour classification in
            annotated data
    classify tracklet as most frequent type
```
-->

## Tracking in Interactions {#sec-interactions}

<!--
### Outline

- problem definition
- data synthesis
- detector
- tracking in interaction
- references: Mobilenets [@Howard2017a]
- figures:
    - synthetic data
    - tracking (few consecutive frames in a grid, `experiments_other/20180301_interaction_tracking/003_hard.mp4`)
-->

Multiple objects tracking is challenging when objects interact or come to proximity. Following the popular tracking by detection scheme we propose a specialised detector for objects in interactions. The CNN based detector is trained on synthetic interactions and outputs positions of all objects in interaction. The detections in consecutive frames are then connected to tracks.

### Data Synthesis

The synthetic interactions are generated from single object tracklets (see Section [#sec-cardinality]). We compose multiple objects from random single object regions. The synthetic data generator uses background subtraction for precise source data segmentation, alpha blending for realistic appearance and parametrized randomized composition to mimic real interactions.

The detector is trained only on the synthetic data. This approach is free of manual annotation labour and easily adaptable to input data with objects of previously unseen appearance.

~ Figure { caption: "Synthetic interactions" }
placeholder
~

### Detector

We take advantage of the constructed tracking graph (see Section [#sec-graph]) that provides us with the number of objects in an interaction. Currently we train specific detectors for interactions with fixed number of objects (e.g. detectors for interactions of 2 and 3 objects). We chose the Mobilenet architecture [@Howard2017a] with good computational resources and accuracy tradeoffs.

The network head comprises of $3n$ regressors, where $n$ is the number of objects in interaction. The three regressed values are planar position $\vec{x}$ and major axis angle $\theta$. For training we use following loss function:
~ Equation
L({\vec{x_i},\theta_i}) = (1-\lambda) \sum_i  \|\vec{x_i} - \vec{x_i^*}\|_1 + \lambda \sum_i e(\theta_i, \theta_i^*),
~
where $\lambda$ is a balancing weight between position and angle error, $\vec{x_i}$ and $\vec{x_j}$ are object positions, $\theta_i$ and $\theta_j$ are object major axis angles and $e$ is direction agnostic absolute difference between two angles. Function $e$ range is $[0,90]$ degrees.

Preliminary experiments shown that the detector trained on the `Camera1` ant dataset generalizes well to other data with elongated objects.

### Tracking

Tracking is performed in a sequential manner. Objects in time $t$ and $t+1$ are connected to tracks .. TODO

~ Equation
c_\mathrm{track}(\{\vec{x_i}, \theta_i\}, \{\vec{x_j}, \theta_j\}) = w \|\vec{x_i} - \vec{x_j}\|_1 + (1-w)  e(\theta_i, \theta_j),
~
where $w$ is balancing weight between position and angle difference, $\vec{x_i}$ and $\vec{x_j}$ are object positions, $\theta_i$ and $\theta_j$ are object major axis angles and $e$ is direction agnostic absolute difference between two angles. Function $e$ range is $[0,90]$ degrees.

<!--
% see scripts/CNN/interactions.py:87
-->

## Re-identification

- complete sets
- track: sequence of tracklets, non contiguous in time

### Probability of tracklets having the same ID set
Each region can be described by low dimensional descriptor. Descriptor is obtained using Siamese CNN trained with a triplet loss.
<!--
Kumar, B. G., Gustavo Carneiro, and Ian Reid. "Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.
-->

Each track is represented by $k$ prototypes $\psi_k$. Prototype is described by two vectors $\vec{\mu_k}$ - mean and $\vec{\sigma_k}$ - standard deviation and by weight $w_k$.
$\Gamma (t_i) $ is an ID set of track $t_i$
~ Equation
 f(t_1, t_2)= \sum_{\psi_1 \in \Psi (t_1)} \sum_{\psi_2 \in \Psi (t_2)} \frac{w_1}{W_1} \cdot 2 \cdot \mathrm{CDF}(\vec{0}, \vec{\sigma_1}, -\left\lVert \vec{\mu_1} - \vec{\mu_2} \right\rVert),
~
where $\mathrm{CDF}$ stands for cumulative density function of a normal distribution.
~ Equation
P(\Gamma (t_1) = \Gamma (t_2)) = \frac{f(t_1, t_2) + f(t_2, t_1)}{2}
~

Pak jeste detail... Je to potreba provest na obe strany... Jakoby (P(t_1 == t_2) + P(t_2 == t_1)) / 2. Musim vystupovat...

# Experiments

- datasets description
  - zebrafish (`5Zebrafish_nocover_22min.avi`)
  - ants1 (Cam1.avi)
  - ants3 (Camera3.avi)
  - sowbug
- evaluation
  - with / without interaction solver
- comparison
- computational time breakdown, speed relative to realtime
- figure: example frames in a grid


~ TableFigure {#tab-results; caption:"TODO"; }
|method   |dataset  |correct|unknown|wrong|
|:--------|:--------|------:|------:|----:|
|ours     |Ants1    |68.46  |31.50  |0.04 |
|idTracker|Ants1    |71.68  |28.01  |0.32 |
|ours     |Sowbug3  |80.14  |12.98  |6.89 |
|idTracker|Sowbug3  |70.60  |14.12  |15.28|
|ours     |Ants3    |89.95  |9.88   |0.17 |
|ours     |Zebrafish|88.14  |9.88   |0.16 |
|idTracker|Ants3    |82.38  |12.37  |5.25 |
|idTracker|Zebrafish|88.00  |11.36  |0.63 |
~

~ Figure { #fig-results; caption:"TODO" }
![results]
~

[results]: figures/results.pdf "results chart" { width:auto; max-width:100%}


<!--
# Future Work

(not necessary included in the paper)


No Matlab licence needed (compared to idTracker, ABCTracker)

- user study
-->

# Conclusion

[BIB]

